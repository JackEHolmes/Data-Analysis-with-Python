{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T04:56:25.884917Z",
     "start_time": "2019-04-27T04:56:25.882319Z"
    }
   },
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T05:16:00.329265Z",
     "start_time": "2019-04-27T04:58:26.930342Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/she/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n",
      "--2019-04-27 12:58:43--  http://mattmahoney.net/dc/text8.zip\n",
      "Resolving mattmahoney.net (mattmahoney.net)... 67.195.197.75\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 31344016 (30M) [application/zip]\n",
      "Saving to: ‘text8.zip’\n",
      "\n",
      "text8.zip            18%[==>                 ]   5.54M  14.3KB/s    in 6m 32s  \n",
      "\n",
      "2019-04-27 13:05:16 (14.5 KB/s) - Connection closed at byte 5806592. Retrying.\n",
      "\n",
      "--2019-04-27 13:05:17--  (try: 2)  http://mattmahoney.net/dc/text8.zip\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 31344016 (30M), 25537424 (24M) remaining [application/zip]\n",
      "Saving to: ‘text8.zip’\n",
      "\n",
      "text8.zip            77%[+++===========>     ]  23.21M  30.5KB/s    in 10m 5s  \n",
      "\n",
      "2019-04-27 13:15:22 (29.9 KB/s) - Connection closed at byte 24333963. Retrying.\n",
      "\n",
      "--2019-04-27 13:15:24--  (try: 3)  http://mattmahoney.net/dc/text8.zip\n",
      "Connecting to mattmahoney.net (mattmahoney.net)|67.195.197.75|:80... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 31344016 (30M), 7010053 (6.7M) remaining [application/zip]\n",
      "Saving to: ‘text8.zip’\n",
      "\n",
      "text8.zip           100%[+++++++++++++++====>]  29.89M   256KB/s    in 34s     \n",
      "\n",
      "2019-04-27 13:15:59 (202 KB/s) - ‘text8.zip’ saved [31344016/31344016]\n",
      "\n",
      "Archive:  text8.zip\n",
      "  inflating: text8                   \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown') \n",
    "\n",
    "# Generate brown corpus text file\n",
    "with open('brown_corp.txt', 'w+') as f:\n",
    "    for word in nltk.corpus.brown.words():\n",
    "        f.write('{word} '.format(word=word))\n",
    "\n",
    "import os.path\n",
    "if not os.path.isfile('text8'):\n",
    "    !wget -c http://mattmahoney.net/dc/text8.zip\n",
    "    !unzip text8.zip\n",
    "# download and preprocess the text9 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T07:44:43.253777Z",
     "start_time": "2019-04-27T07:44:34.712328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:44:34,854 : INFO : collecting all words and their counts\n",
      "2019-04-27 15:44:34,859 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec on brown_corp.txt corpus..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:44:35,157 : INFO : collected 56057 word types from a corpus of 1161192 raw words and 117 sentences\n",
      "2019-04-27 15:44:35,157 : INFO : Loading a fresh vocabulary\n",
      "2019-04-27 15:44:35,370 : INFO : min_count=5 retains 15173 unique words (27% of original 56057, drops 40884)\n",
      "2019-04-27 15:44:35,371 : INFO : min_count=5 leaves 1095086 word corpus (94% of original 1161192, drops 66106)\n",
      "2019-04-27 15:44:35,411 : INFO : deleting the raw counts dictionary of 56057 items\n",
      "2019-04-27 15:44:35,414 : INFO : sample=0.0001 downsamples 340 most-common words\n",
      "2019-04-27 15:44:35,415 : INFO : downsampling leaves estimated 540252 word corpus (49.3% of prior 1095086)\n",
      "2019-04-27 15:44:35,457 : INFO : estimated required memory for 15173 words and 100 dimensions: 19724900 bytes\n",
      "2019-04-27 15:44:35,458 : INFO : resetting layer weights\n",
      "2019-04-27 15:44:35,624 : INFO : training model with 3 workers on 15173 vocabulary and 100 features, using sg=1 hs=0 sample=0.0001 negative=5 window=5\n",
      "2019-04-27 15:44:36,637 : INFO : EPOCH 1 - PROGRESS: at 67.52% examples, 370181 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:44:37,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:44:37,134 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:44:37,151 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:44:37,151 : INFO : EPOCH - 1 : training on 1161192 raw words (539923 effective words) took 1.5s, 354215 effective words/s\n",
      "2019-04-27 15:44:38,162 : INFO : EPOCH 2 - PROGRESS: at 77.78% examples, 425244 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:44:38,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:44:38,400 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:44:38,410 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:44:38,410 : INFO : EPOCH - 2 : training on 1161192 raw words (539814 effective words) took 1.3s, 429480 effective words/s\n",
      "2019-04-27 15:44:39,414 : INFO : EPOCH 3 - PROGRESS: at 79.49% examples, 437974 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:44:39,624 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:44:39,635 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:44:39,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:44:39,644 : INFO : EPOCH - 3 : training on 1161192 raw words (540489 effective words) took 1.2s, 438904 effective words/s\n",
      "2019-04-27 15:44:40,658 : INFO : EPOCH 4 - PROGRESS: at 75.21% examples, 411714 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:44:40,915 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:44:40,934 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:44:40,936 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:44:40,937 : INFO : EPOCH - 4 : training on 1161192 raw words (540485 effective words) took 1.3s, 418806 effective words/s\n",
      "2019-04-27 15:44:41,948 : INFO : EPOCH 5 - PROGRESS: at 72.65% examples, 398735 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:44:42,233 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:44:42,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:44:42,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:44:42,249 : INFO : EPOCH - 5 : training on 1161192 raw words (539887 effective words) took 1.3s, 412538 effective words/s\n",
      "2019-04-27 15:44:42,249 : INFO : training on a 5805960 raw words (2700598 effective words) took 6.6s, 407686 effective words/s\n",
      "2019-04-27 15:44:42,250 : INFO : storing 15173x100 projection weights into models/brown_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.1 s, sys: 284 ms, total: 20.4 s\n",
      "Wall time: 7.4 s\n",
      "\n",
      "Saved gensim model as brown_gs.vec\n"
     ]
    }
   ],
   "source": [
    "MODELS_DIR = 'models/'\n",
    "!mkdir -p {MODELS_DIR}\n",
    "\n",
    "lr = 0.05\n",
    "dim = 100\n",
    "ws = 5\n",
    "epoch = 5\n",
    "minCount = 5\n",
    "neg = 5\n",
    "loss = 'ns'\n",
    "t = 1e-4\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "params = {\n",
    "    'alpha': lr,\n",
    "    'size': dim,\n",
    "    'window': ws,\n",
    "    'iter': epoch,\n",
    "    'min_count': minCount,\n",
    "    'sample': t,\n",
    "    'sg': 1,\n",
    "    'hs': 0,\n",
    "    'negative': neg\n",
    "}\n",
    "\n",
    "def train_models(corpus_file, output_name):        \n",
    "    output_file = '{:s}_gs'.format(output_name)\n",
    "    if not os.path.isfile(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file))):\n",
    "        print('\\nTraining word2vec on {:s} corpus..'.format(corpus_file))\n",
    "        \n",
    "        # Text8Corpus class for reading space-separated words file\n",
    "        %time gs_model = Word2Vec(Text8Corpus(corpus_file), **params); gs_model\n",
    "        # Direct local variable lookup doesn't work properly with magic statements (%time)\n",
    "        locals()['gs_model'].wv.save_word2vec_format(os.path.join(MODELS_DIR, '{:s}.vec'.format(output_file)))\n",
    "        print('\\nSaved gensim model as {:s}.vec'.format(output_file))\n",
    "    else:\n",
    "        print('\\nUsing existing model file {:s}.vec'.format(output_file))\n",
    "\n",
    "evaluation_data = {}\n",
    "train_models('brown_corp.txt', 'brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T07:49:49.435769Z",
     "start_time": "2019-04-27T07:46:47.570594Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:46:47,574 : INFO : collecting all words and their counts\n",
      "2019-04-27 15:46:47,579 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training word2vec on text8 corpus..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:46:52,206 : INFO : collected 253854 word types from a corpus of 17005207 raw words and 1701 sentences\n",
      "2019-04-27 15:46:52,206 : INFO : Loading a fresh vocabulary\n",
      "2019-04-27 15:46:52,564 : INFO : min_count=5 retains 71290 unique words (28% of original 253854, drops 182564)\n",
      "2019-04-27 15:46:52,566 : INFO : min_count=5 leaves 16718844 word corpus (98% of original 17005207, drops 286363)\n",
      "2019-04-27 15:46:52,750 : INFO : deleting the raw counts dictionary of 253854 items\n",
      "2019-04-27 15:46:52,758 : INFO : sample=0.0001 downsamples 341 most-common words\n",
      "2019-04-27 15:46:52,758 : INFO : downsampling leaves estimated 9386181 word corpus (56.1% of prior 16718844)\n",
      "2019-04-27 15:46:52,935 : INFO : estimated required memory for 71290 words and 100 dimensions: 92677000 bytes\n",
      "2019-04-27 15:46:52,935 : INFO : resetting layer weights\n",
      "2019-04-27 15:46:53,605 : INFO : training model with 3 workers on 71290 vocabulary and 100 features, using sg=1 hs=0 sample=0.0001 negative=5 window=5\n",
      "2019-04-27 15:46:54,609 : INFO : EPOCH 1 - PROGRESS: at 3.59% examples, 336471 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:46:55,615 : INFO : EPOCH 1 - PROGRESS: at 7.29% examples, 335426 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:46:56,631 : INFO : EPOCH 1 - PROGRESS: at 11.17% examples, 341209 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:46:57,639 : INFO : EPOCH 1 - PROGRESS: at 14.76% examples, 339866 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:46:58,658 : INFO : EPOCH 1 - PROGRESS: at 18.64% examples, 343481 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:46:59,672 : INFO : EPOCH 1 - PROGRESS: at 22.52% examples, 346086 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:00,678 : INFO : EPOCH 1 - PROGRESS: at 26.28% examples, 347879 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:01,679 : INFO : EPOCH 1 - PROGRESS: at 29.69% examples, 345388 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:02,686 : INFO : EPOCH 1 - PROGRESS: at 32.86% examples, 340570 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:03,689 : INFO : EPOCH 1 - PROGRESS: at 35.80% examples, 334486 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:04,701 : INFO : EPOCH 1 - PROGRESS: at 39.15% examples, 332340 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:05,715 : INFO : EPOCH 1 - PROGRESS: at 42.62% examples, 331651 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:06,726 : INFO : EPOCH 1 - PROGRESS: at 45.80% examples, 328868 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:07,730 : INFO : EPOCH 1 - PROGRESS: at 49.56% examples, 330728 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:08,735 : INFO : EPOCH 1 - PROGRESS: at 52.67% examples, 328057 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:09,747 : INFO : EPOCH 1 - PROGRESS: at 56.03% examples, 327377 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:10,784 : INFO : EPOCH 1 - PROGRESS: at 59.61% examples, 327256 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:11,788 : INFO : EPOCH 1 - PROGRESS: at 62.73% examples, 325306 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:12,789 : INFO : EPOCH 1 - PROGRESS: at 65.73% examples, 323019 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:13,793 : INFO : EPOCH 1 - PROGRESS: at 68.37% examples, 319319 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:14,797 : INFO : EPOCH 1 - PROGRESS: at 71.13% examples, 316593 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:15,841 : INFO : EPOCH 1 - PROGRESS: at 74.43% examples, 315623 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:16,845 : INFO : EPOCH 1 - PROGRESS: at 77.19% examples, 312297 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:17,857 : INFO : EPOCH 1 - PROGRESS: at 79.89% examples, 309551 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:18,860 : INFO : EPOCH 1 - PROGRESS: at 82.60% examples, 307251 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:19,874 : INFO : EPOCH 1 - PROGRESS: at 85.24% examples, 304901 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:20,875 : INFO : EPOCH 1 - PROGRESS: at 87.77% examples, 302425 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:21,878 : INFO : EPOCH 1 - PROGRESS: at 90.53% examples, 301043 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:22,882 : INFO : EPOCH 1 - PROGRESS: at 93.06% examples, 298695 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:23,900 : INFO : EPOCH 1 - PROGRESS: at 95.77% examples, 297006 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:24,912 : INFO : EPOCH 1 - PROGRESS: at 98.41% examples, 295134 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:25,411 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:47:25,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:47:25,420 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:47:25,421 : INFO : EPOCH - 1 : training on 17005207 raw words (9388642 effective words) took 31.8s, 295108 effective words/s\n",
      "2019-04-27 15:47:26,435 : INFO : EPOCH 2 - PROGRESS: at 2.59% examples, 242917 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:27,469 : INFO : EPOCH 2 - PROGRESS: at 5.11% examples, 234133 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:28,474 : INFO : EPOCH 2 - PROGRESS: at 7.70% examples, 233502 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:29,478 : INFO : EPOCH 2 - PROGRESS: at 10.46% examples, 238801 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:30,489 : INFO : EPOCH 2 - PROGRESS: at 13.05% examples, 238784 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:31,527 : INFO : EPOCH 2 - PROGRESS: at 16.23% examples, 247067 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:32,536 : INFO : EPOCH 2 - PROGRESS: at 18.69% examples, 244581 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:33,573 : INFO : EPOCH 2 - PROGRESS: at 20.81% examples, 237700 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:34,610 : INFO : EPOCH 2 - PROGRESS: at 23.16% examples, 235219 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:35,611 : INFO : EPOCH 2 - PROGRESS: at 25.69% examples, 235827 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:36,625 : INFO : EPOCH 2 - PROGRESS: at 28.45% examples, 237968 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:37,663 : INFO : EPOCH 2 - PROGRESS: at 31.51% examples, 241839 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:38,675 : INFO : EPOCH 2 - PROGRESS: at 34.51% examples, 244968 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:39,691 : INFO : EPOCH 2 - PROGRESS: at 37.92% examples, 250189 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:40,692 : INFO : EPOCH 2 - PROGRESS: at 41.39% examples, 255017 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:41,699 : INFO : EPOCH 2 - PROGRESS: at 44.80% examples, 259132 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:42,707 : INFO : EPOCH 2 - PROGRESS: at 48.21% examples, 262632 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:43,726 : INFO : EPOCH 2 - PROGRESS: at 50.91% examples, 262022 words/s, in_qsize 5, out_qsize 1\n",
      "2019-04-27 15:47:44,748 : INFO : EPOCH 2 - PROGRESS: at 53.73% examples, 261996 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:45,785 : INFO : EPOCH 2 - PROGRESS: at 56.38% examples, 261000 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:46,804 : INFO : EPOCH 2 - PROGRESS: at 59.08% examples, 260425 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:47,820 : INFO : EPOCH 2 - PROGRESS: at 61.90% examples, 260480 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:48,834 : INFO : EPOCH 2 - PROGRESS: at 64.61% examples, 260058 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:49,857 : INFO : EPOCH 2 - PROGRESS: at 67.84% examples, 261650 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:50,860 : INFO : EPOCH 2 - PROGRESS: at 70.66% examples, 261849 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:51,885 : INFO : EPOCH 2 - PROGRESS: at 73.13% examples, 260513 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:52,888 : INFO : EPOCH 2 - PROGRESS: at 76.43% examples, 261722 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:53,912 : INFO : EPOCH 2 - PROGRESS: at 79.66% examples, 262636 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:54,921 : INFO : EPOCH 2 - PROGRESS: at 82.77% examples, 263499 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:55,929 : INFO : EPOCH 2 - PROGRESS: at 85.60% examples, 263543 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:47:56,945 : INFO : EPOCH 2 - PROGRESS: at 88.30% examples, 263119 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:47:57,967 : INFO : EPOCH 2 - PROGRESS: at 91.06% examples, 262910 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:47:58,976 : INFO : EPOCH 2 - PROGRESS: at 93.47% examples, 261645 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:48:00,009 : INFO : EPOCH 2 - PROGRESS: at 95.94% examples, 260533 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:01,030 : INFO : EPOCH 2 - PROGRESS: at 98.06% examples, 258490 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:01,697 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:48:01,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:48:01,709 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:48:01,709 : INFO : EPOCH - 2 : training on 17005207 raw words (9385545 effective words) took 36.3s, 258660 effective words/s\n",
      "2019-04-27 15:48:02,757 : INFO : EPOCH 3 - PROGRESS: at 2.76% examples, 250406 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:03,770 : INFO : EPOCH 3 - PROGRESS: at 5.64% examples, 255465 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:04,806 : INFO : EPOCH 3 - PROGRESS: at 8.23% examples, 245915 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:05,823 : INFO : EPOCH 3 - PROGRESS: at 11.29% examples, 253737 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:06,862 : INFO : EPOCH 3 - PROGRESS: at 13.82% examples, 249028 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:07,887 : INFO : EPOCH 3 - PROGRESS: at 15.52% examples, 233533 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:08,925 : INFO : EPOCH 3 - PROGRESS: at 17.46% examples, 225071 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-27 15:48:09,927 : INFO : EPOCH 3 - PROGRESS: at 19.22% examples, 217724 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:10,937 : INFO : EPOCH 3 - PROGRESS: at 21.46% examples, 216678 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:11,942 : INFO : EPOCH 3 - PROGRESS: at 23.69% examples, 216295 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:12,952 : INFO : EPOCH 3 - PROGRESS: at 25.87% examples, 215280 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:13,955 : INFO : EPOCH 3 - PROGRESS: at 28.04% examples, 214594 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:14,986 : INFO : EPOCH 3 - PROGRESS: at 30.98% examples, 219187 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:15,995 : INFO : EPOCH 3 - PROGRESS: at 34.16% examples, 224979 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:17,036 : INFO : EPOCH 3 - PROGRESS: at 37.04% examples, 227582 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:48:18,047 : INFO : EPOCH 3 - PROGRESS: at 39.86% examples, 229692 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:19,059 : INFO : EPOCH 3 - PROGRESS: at 42.45% examples, 230376 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:20,088 : INFO : EPOCH 3 - PROGRESS: at 44.03% examples, 225565 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:48:21,137 : INFO : EPOCH 3 - PROGRESS: at 45.97% examples, 222809 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:22,138 : INFO : EPOCH 3 - PROGRESS: at 47.97% examples, 221138 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:23,154 : INFO : EPOCH 3 - PROGRESS: at 49.97% examples, 219530 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:24,160 : INFO : EPOCH 3 - PROGRESS: at 52.56% examples, 220466 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:25,162 : INFO : EPOCH 3 - PROGRESS: at 55.56% examples, 223309 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:26,168 : INFO : EPOCH 3 - PROGRESS: at 58.79% examples, 226548 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:27,177 : INFO : EPOCH 3 - PROGRESS: at 62.08% examples, 229763 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:28,177 : INFO : EPOCH 3 - PROGRESS: at 65.26% examples, 232351 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:29,189 : INFO : EPOCH 3 - PROGRESS: at 67.37% examples, 231038 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:48:30,240 : INFO : EPOCH 3 - PROGRESS: at 69.14% examples, 228395 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:31,298 : INFO : EPOCH 3 - PROGRESS: at 71.25% examples, 227022 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:32,367 : INFO : EPOCH 3 - PROGRESS: at 73.37% examples, 225591 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:33,384 : INFO : EPOCH 3 - PROGRESS: at 75.31% examples, 223843 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:34,402 : INFO : EPOCH 3 - PROGRESS: at 77.90% examples, 223861 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:35,417 : INFO : EPOCH 3 - PROGRESS: at 80.48% examples, 224250 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:36,437 : INFO : EPOCH 3 - PROGRESS: at 83.48% examples, 225776 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:37,471 : INFO : EPOCH 3 - PROGRESS: at 86.65% examples, 227595 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:38,477 : INFO : EPOCH 3 - PROGRESS: at 89.83% examples, 229572 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:39,492 : INFO : EPOCH 3 - PROGRESS: at 93.00% examples, 231218 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:40,511 : INFO : EPOCH 3 - PROGRESS: at 96.12% examples, 232636 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:41,520 : INFO : EPOCH 3 - PROGRESS: at 99.35% examples, 234247 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:41,685 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:48:41,701 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:48:41,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:48:41,707 : INFO : EPOCH - 3 : training on 17005207 raw words (9384875 effective words) took 40.0s, 234655 effective words/s\n",
      "2019-04-27 15:48:42,713 : INFO : EPOCH 4 - PROGRESS: at 2.94% examples, 277645 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:43,714 : INFO : EPOCH 4 - PROGRESS: at 6.11% examples, 283116 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:44,724 : INFO : EPOCH 4 - PROGRESS: at 9.35% examples, 286825 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:45,748 : INFO : EPOCH 4 - PROGRESS: at 12.58% examples, 288746 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:46,756 : INFO : EPOCH 4 - PROGRESS: at 15.76% examples, 290273 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:47,773 : INFO : EPOCH 4 - PROGRESS: at 19.05% examples, 292295 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:48,808 : INFO : EPOCH 4 - PROGRESS: at 22.28% examples, 292525 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:49,838 : INFO : EPOCH 4 - PROGRESS: at 25.46% examples, 292872 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:50,839 : INFO : EPOCH 4 - PROGRESS: at 28.63% examples, 293969 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:51,844 : INFO : EPOCH 4 - PROGRESS: at 31.69% examples, 293876 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:52,858 : INFO : EPOCH 4 - PROGRESS: at 34.86% examples, 294394 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:53,869 : INFO : EPOCH 4 - PROGRESS: at 38.04% examples, 294640 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:54,880 : INFO : EPOCH 4 - PROGRESS: at 41.21% examples, 294537 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:55,890 : INFO : EPOCH 4 - PROGRESS: at 44.39% examples, 294734 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:56,897 : INFO : EPOCH 4 - PROGRESS: at 47.56% examples, 294960 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:57,913 : INFO : EPOCH 4 - PROGRESS: at 50.73% examples, 295080 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-27 15:48:58,914 : INFO : EPOCH 4 - PROGRESS: at 53.91% examples, 295396 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:48:59,928 : INFO : EPOCH 4 - PROGRESS: at 57.14% examples, 295734 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:00,950 : INFO : EPOCH 4 - PROGRESS: at 60.32% examples, 295581 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:01,955 : INFO : EPOCH 4 - PROGRESS: at 63.61% examples, 296240 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:02,978 : INFO : EPOCH 4 - PROGRESS: at 66.84% examples, 296236 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:03,985 : INFO : EPOCH 4 - PROGRESS: at 70.02% examples, 296314 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:49:04,987 : INFO : EPOCH 4 - PROGRESS: at 73.25% examples, 296680 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:05,988 : INFO : EPOCH 4 - PROGRESS: at 76.54% examples, 296544 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:06,991 : INFO : EPOCH 4 - PROGRESS: at 79.78% examples, 296447 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:07,997 : INFO : EPOCH 4 - PROGRESS: at 83.07% examples, 296804 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:09,023 : INFO : EPOCH 4 - PROGRESS: at 86.30% examples, 296803 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:10,034 : INFO : EPOCH 4 - PROGRESS: at 89.48% examples, 296842 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:11,042 : INFO : EPOCH 4 - PROGRESS: at 92.71% examples, 296929 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:12,056 : INFO : EPOCH 4 - PROGRESS: at 96.00% examples, 297159 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:13,067 : INFO : EPOCH 4 - PROGRESS: at 99.24% examples, 297087 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:13,278 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:49:13,281 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:49:13,294 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:49:13,295 : INFO : EPOCH - 4 : training on 17005207 raw words (9386620 effective words) took 31.6s, 297195 effective words/s\n",
      "2019-04-27 15:49:14,312 : INFO : EPOCH 5 - PROGRESS: at 3.12% examples, 290579 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:15,329 : INFO : EPOCH 5 - PROGRESS: at 6.53% examples, 296983 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:49:16,349 : INFO : EPOCH 5 - PROGRESS: at 9.76% examples, 295684 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:17,356 : INFO : EPOCH 5 - PROGRESS: at 13.05% examples, 298059 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:18,373 : INFO : EPOCH 5 - PROGRESS: at 16.28% examples, 298148 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:19,375 : INFO : EPOCH 5 - PROGRESS: at 19.46% examples, 297982 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:20,383 : INFO : EPOCH 5 - PROGRESS: at 22.75% examples, 299379 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:21,389 : INFO : EPOCH 5 - PROGRESS: at 25.93% examples, 299837 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:49:22,397 : INFO : EPOCH 5 - PROGRESS: at 29.04% examples, 299192 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:23,404 : INFO : EPOCH 5 - PROGRESS: at 32.22% examples, 299613 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:24,407 : INFO : EPOCH 5 - PROGRESS: at 35.39% examples, 299870 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:25,419 : INFO : EPOCH 5 - PROGRESS: at 38.62% examples, 299881 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:26,426 : INFO : EPOCH 5 - PROGRESS: at 41.86% examples, 300175 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:27,440 : INFO : EPOCH 5 - PROGRESS: at 45.09% examples, 300212 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:28,449 : INFO : EPOCH 5 - PROGRESS: at 48.27% examples, 300071 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:29,473 : INFO : EPOCH 5 - PROGRESS: at 51.50% examples, 299894 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:30,496 : INFO : EPOCH 5 - PROGRESS: at 54.67% examples, 299729 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:31,503 : INFO : EPOCH 5 - PROGRESS: at 57.85% examples, 299569 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:32,514 : INFO : EPOCH 5 - PROGRESS: at 61.14% examples, 299964 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:33,542 : INFO : EPOCH 5 - PROGRESS: at 64.37% examples, 299760 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:34,549 : INFO : EPOCH 5 - PROGRESS: at 67.61% examples, 299854 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-27 15:49:35,552 : INFO : EPOCH 5 - PROGRESS: at 70.78% examples, 299858 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:36,554 : INFO : EPOCH 5 - PROGRESS: at 73.96% examples, 299823 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:37,569 : INFO : EPOCH 5 - PROGRESS: at 77.37% examples, 299612 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:38,596 : INFO : EPOCH 5 - PROGRESS: at 80.72% examples, 299718 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:39,603 : INFO : EPOCH 5 - PROGRESS: at 83.89% examples, 299616 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:40,603 : INFO : EPOCH 5 - PROGRESS: at 87.07% examples, 299528 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:41,626 : INFO : EPOCH 5 - PROGRESS: at 90.30% examples, 299601 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:42,631 : INFO : EPOCH 5 - PROGRESS: at 93.53% examples, 299512 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-27 15:49:43,634 : INFO : EPOCH 5 - PROGRESS: at 96.77% examples, 299554 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-27 15:49:44,618 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-27 15:49:44,622 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-27 15:49:44,631 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-27 15:49:44,631 : INFO : EPOCH - 5 : training on 17005207 raw words (9386827 effective words) took 31.3s, 299577 effective words/s\n",
      "2019-04-27 15:49:44,632 : INFO : training on a 85026035 raw words (46932509 effective words) took 171.0s, 274418 effective words/s\n",
      "2019-04-27 15:49:44,634 : INFO : storing 71290x100 projection weights into models/text8_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 32s, sys: 4.83 s, total: 8min 37s\n",
      "Wall time: 2min 57s\n",
      "\n",
      "Saved gensim model as text8_gs.vec\n"
     ]
    }
   ],
   "source": [
    "train_models(corpus_file='text8', output_name='text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T07:39:58.435178Z",
     "start_time": "2019-04-27T07:39:05.776244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-27 15:39:05--  https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.108.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 603955 (590K) [text/plain]\n",
      "Saving to: ‘questions-words.txt’\n",
      "\n",
      "questions-words.txt 100%[===================>] 589.80K  10.3KB/s    in 51s     \n",
      "\n",
      "2019-04-27 15:39:58 (11.5 KB/s) - ‘questions-words.txt’ saved [603955/603955]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# download the file questions-words.txt to be used for comparing word embeddings\n",
    "!wget https://raw.githubusercontent.com/tmikolov/word2vec/master/questions-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T07:51:07.451324Z",
     "start_time": "2019-04-27T07:50:56.847588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:50:56,860 : INFO : loading projection weights from models/brown_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Gensim embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:50:58,119 : INFO : loaded (15173, 100) matrix from models/brown_gs.vec\n",
      "2019-04-27 15:50:58,231 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-04-27 15:50:58,386 : INFO : capital-common-countries: 4.4% (4/90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Word2Vec:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:50:58,471 : INFO : capital-world: 0.0% (0/44)\n",
      "2019-04-27 15:50:58,498 : INFO : currency: 0.0% (0/12)\n",
      "2019-04-27 15:50:59,219 : INFO : city-in-state: 0.9% (4/457)\n",
      "2019-04-27 15:50:59,536 : INFO : family: 16.2% (34/210)\n",
      "2019-04-27 15:51:00,758 : INFO : gram1-adjective-to-adverb: 0.0% (0/756)\n",
      "2019-04-27 15:51:00,930 : INFO : gram2-opposite: 0.0% (0/132)\n",
      "2019-04-27 15:51:02,268 : INFO : gram3-comparative: 1.4% (15/1056)\n",
      "2019-04-27 15:51:02,533 : INFO : gram4-superlative: 1.0% (2/210)\n",
      "2019-04-27 15:51:03,419 : INFO : gram5-present-participle: 0.2% (1/650)\n",
      "2019-04-27 15:51:03,919 : INFO : gram6-nationality-adjective: 1.7% (5/297)\n",
      "2019-04-27 15:51:05,935 : INFO : gram7-past-tense: 1.8% (23/1260)\n",
      "2019-04-27 15:51:06,800 : INFO : gram8-plural: 4.2% (23/552)\n",
      "2019-04-27 15:51:07,443 : INFO : gram9-plural-verbs: 0.9% (3/342)\n",
      "2019-04-27 15:51:07,444 : INFO : total: 1.9% (114/6068)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 42/813, Accuracy: 5.17%\n",
      "Syntactic: 72/5255, Accuracy: 1.37%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from  gensim.models import KeyedVectors\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Training times in seconds\n",
    "evaluation_data['brown'] = [(18, 54.3, 32.5)]\n",
    "evaluation_data['text8'] = [(402, 942, 496)]\n",
    "evaluation_data['text9'] = [(3218, 6589, 3550)]\n",
    "\n",
    "def print_accuracy(model, questions_file):\n",
    "    print('Evaluating...\\n')\n",
    "    acc = model.accuracy(questions_file)\n",
    "\n",
    "    sem_correct = sum((len(acc[i]['correct']) for i in range(5)))\n",
    "    sem_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5))\n",
    "    sem_acc = 100*float(sem_correct)/sem_total\n",
    "    print('\\nSemantic: {:d}/{:d}, Accuracy: {:.2f}%'.format(sem_correct, sem_total, sem_acc))\n",
    "    \n",
    "    syn_correct = sum((len(acc[i]['correct']) for i in range(5, len(acc)-1)))\n",
    "    syn_total = sum((len(acc[i]['correct']) + len(acc[i]['incorrect'])) for i in range(5,len(acc)-1))\n",
    "    syn_acc = 100*float(syn_correct)/syn_total\n",
    "    print('Syntactic: {:d}/{:d}, Accuracy: {:.2f}%\\n'.format(syn_correct, syn_total, syn_acc))\n",
    "    return (sem_acc, syn_acc)\n",
    "\n",
    "word_analogies_file = 'questions-words.txt'\n",
    "accuracies = []\n",
    "print('\\nLoading Gensim embeddings')\n",
    "brown_gs = KeyedVectors.load_word2vec_format(MODELS_DIR + 'brown_gs.vec')\n",
    "print('Accuracy for Word2Vec:')\n",
    "accuracies.append(print_accuracy(brown_gs, word_analogies_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-27T07:51:55.645768Z",
     "start_time": "2019-04-27T07:51:07.456967Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:51:07,462 : INFO : loading projection weights from models/text8_gs.vec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gensim embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:51:14,199 : INFO : loaded (71290, 100) matrix from models/text8_gs.vec\n",
      "2019-04-27 15:51:14,298 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for word2vec:\n",
      "Evaluating...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-27 15:51:16,216 : INFO : capital-common-countries: 68.4% (346/506)\n",
      "2019-04-27 15:51:21,867 : INFO : capital-world: 46.0% (668/1452)\n",
      "2019-04-27 15:51:22,934 : INFO : currency: 20.9% (56/268)\n",
      "2019-04-27 15:51:28,494 : INFO : city-in-state: 27.6% (433/1571)\n",
      "2019-04-27 15:51:29,539 : INFO : family: 56.2% (172/306)\n",
      "2019-04-27 15:51:32,043 : INFO : gram1-adjective-to-adverb: 14.4% (109/756)\n",
      "2019-04-27 15:51:33,087 : INFO : gram2-opposite: 10.5% (32/306)\n",
      "2019-04-27 15:51:37,372 : INFO : gram3-comparative: 52.1% (657/1260)\n",
      "2019-04-27 15:51:39,122 : INFO : gram4-superlative: 31.4% (159/506)\n",
      "2019-04-27 15:51:42,385 : INFO : gram5-present-participle: 23.6% (234/992)\n",
      "2019-04-27 15:51:46,634 : INFO : gram6-nationality-adjective: 79.1% (1085/1371)\n",
      "2019-04-27 15:51:50,794 : INFO : gram7-past-tense: 31.5% (420/1332)\n",
      "2019-04-27 15:51:53,685 : INFO : gram8-plural: 45.7% (453/992)\n",
      "2019-04-27 15:51:55,638 : INFO : gram9-plural-verbs: 29.1% (189/650)\n",
      "2019-04-27 15:51:55,639 : INFO : total: 40.9% (5013/12268)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic: 1675/4103, Accuracy: 40.82%\n",
      "Syntactic: 3338/8165, Accuracy: 40.88%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracies = []\n",
    "print('Loading Gensim embeddings')\n",
    "text8_gs = KeyedVectors.load_word2vec_format(MODELS_DIR + 'text8_gs.vec')\n",
    "print('Accuracy for word2vec:')\n",
    "accuracies.append(print_accuracy(text8_gs, word_analogies_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
